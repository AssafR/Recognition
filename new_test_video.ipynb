{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test pre-trained RGB model on a single video.\n",
    "\n",
    "Date: 01/15/18\n",
    "Authors: Bolei Zhou and Alex Andonian\n",
    "\n",
    "This script accepts an mp4 video as the command line argument --video_file\n",
    "and averages ResNet50 (trained on Moments) predictions on num_segment equally\n",
    "spaced frames (extracted using ffmpeg).\n",
    "\n",
    "Alternatively, one may instead provide the path to a directory containing\n",
    "video frames saved as jpgs, which are sorted and forwarded through the model.\n",
    "\n",
    "ResNet50 trained on Moments is used to predict the action for each frame,\n",
    "and these class probabilities are average to produce a video-level predction.\n",
    "\n",
    "Optionally, one can generate a new video --rendered_output from the frames\n",
    "used to make the prediction with the predicted category in the top-left corner.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import moviepy.editor as mpy\n",
    "import time\n",
    "\n",
    "import torch.optim\n",
    "import torch.nn.parallel\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "sys.path.append(\"../moments_models/\")\n",
    "\n",
    "from prj_utils import *\n",
    "import pygame\n",
    "\n",
    "import numpy as np \n",
    "import cv2\n",
    "\n",
    "\n",
    "import models\n",
    "\n",
    "#from utils import extract_frames, load_frames, render_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpe\n",
    "video = mpe.VideoFileClip('mrbean.mp4')\n",
    "#np_frame = video.get_frame(2)\n",
    "#np_frame.ipython_display()\n",
    "# n_frames = sum(1 for x in video.iter_frames())\n",
    "# print(n_frames)\n",
    "len=0\n",
    "\n",
    "# images=[]\n",
    "# for frame in video.iter_frames():\n",
    "#     new_im = Image.fromarray(frame)\n",
    "#     images.append(new_im)\n",
    "#     display(new_im)\n",
    "#     len=len+1\n",
    "#     if len>100:\n",
    "#         break;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "framenumbers = range(50,75)\n",
    "frames = [Image.fromarray(video.get_frame(i)).convert('RGB') for i in framenumbers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "arch = 'resnet3d50'\n",
    "model = models.load_model(arch)\n",
    "transform = models.load_transform()\n",
    "\n",
    "# Get dataset categories\n",
    "categories = models.load_categories()\n",
    "eat_idx = categories.index(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# video_file = \"mrbean.mp4\"\n",
    "\n",
    "# # Load the video frame transform\n",
    "# transform = models.load_transform()\n",
    "\n",
    "# frames = extract_frames(video_file, 50)\n",
    "\n",
    "# # [num_frames, 3, 224, 224]\n",
    "t = transform(frames[0])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 25, 224, 224])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if arch == 'resnet3d50':\n",
    "    # [1, num_frames, 3, 224, 224]\n",
    "    inp = torch.stack([transform(frame) for frame in frames], 1).unsqueeze(0)\n",
    "else:\n",
    "    # [num_frames, 3, 224, 224]\n",
    "    inp = torch.stack([transform(frame) for frame in frames])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inp,models):\n",
    "    # Make video prediction\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(inp)\n",
    "        probs = F.softmax(logits, 1)\n",
    "        end = time.time()\n",
    "        return probs, (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([1, 3, 25, 224, 224]), time for prediction: 7.458142280578613 sec\n"
     ]
    }
   ],
   "source": [
    "probs,total = predict(inp,models)\n",
    "print(f'Input size: {inp.shape}, time for prediction: {total} sec')\n",
    "\n",
    "# Convert from Tensor to list\n",
    "eat_probs = probs[:,eat_idx].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1267])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:,eat_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-71d080726997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meat_probs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrendered_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_frames_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageSequenceClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrendered_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#animation = mpy.ipython_display(clip)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/Recognition/prj_utils.py\u001b[0m in \u001b[0;36mrender_frames_2\u001b[0;34m(frames, predictions)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         cv2.putText(img, predictions[index],\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Output the prediction.\n",
    "# video_name = args.frame_folder if args.frame_folder is not None else args.video_file\n",
    "# print('RESULT ON ' + video_name)\n",
    "# for i in range(0, 5):\n",
    "#     print('{:.3f} -> {}'.format(probs[i], categories[idx[i]]))\n",
    "\n",
    "# Render output frames with prediction text.\n",
    "\n",
    "prediction = [str(round(i,5)) for i in eat_probs]\n",
    "rendered_frames = render_frames_2(frames, prediction)\n",
    "clip = mpy.ImageSequenceClip(rendered_frames, fps=1)\n",
    "#animation = mpy.ipython_display(clip)\n",
    "clip.ipython_display(fps=10, loop=1, autoplay=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
